#!/usr/bin/env python3
"""
DILI-Concentration Accuracy Assessment

Comprehensive evaluation of the integrated DILI-concentration prediction system accuracy,
including comparison with previous approaches and clinical validation metrics.
"""

import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent.parent.parent))

import pandas as pd
import numpy as np
from typing import Dict, List, Tuple
import warnings
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score, 
    roc_auc_score, mean_absolute_error, r2_score, classification_report
)
from sklearn.model_selection import cross_val_score, LeaveOneGroupOut
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor

from scripts.analysis.integrated_dili_concentration_prediction import IntegratedDILIConcentrationPredictor

warnings.filterwarnings('ignore')


class DILIConcentrationAccuracyEvaluator:
    """Comprehensive accuracy assessment for DILI-concentration predictions."""
    
    def __init__(self):
        self.predictor = IntegratedDILIConcentrationPredictor()
        self.accuracy_results = {}
        
    def evaluate_comprehensive_accuracy(self, max_drugs: int = 30) -> Dict:
        """Evaluate accuracy across multiple prediction tasks and approaches."""
        print("ðŸŽ¯ COMPREHENSIVE DILI-CONCENTRATION ACCURACY EVALUATION")
        print("=" * 70)
        
        # Get features and data
        features_df = self.predictor.extract_dili_specific_features(max_drugs)
        
        if features_df.empty:
            print("âŒ No data available for accuracy evaluation")
            return {}
        
        print(f"\nðŸ“Š EVALUATION DATASET:")
        print(f"  â€¢ {len(features_df)} drug-concentration combinations")
        print(f"  â€¢ {features_df['drug'].nunique()} unique drugs")
        print(f"  â€¢ {len(features_df.columns)} total features")
        
        accuracy_results = {}
        
        # 1. DILI Classification Accuracy (Original Categories)
        print(f"\n1ï¸âƒ£ DILI CLASSIFICATION ACCURACY:")
        dili_class_accuracy = self._evaluate_dili_classification(features_df)
        accuracy_results['dili_classification'] = dili_class_accuracy
        
        # 2. Concentration Threshold Prediction Accuracy
        print(f"\n2ï¸âƒ£ CONCENTRATION THRESHOLD PREDICTION:")
        threshold_accuracy = self._evaluate_threshold_prediction(features_df)
        accuracy_results['threshold_prediction'] = threshold_accuracy
        
        # 3. Safety Margin Prediction Accuracy
        print(f"\n3ï¸âƒ£ SAFETY MARGIN PREDICTION:")
        safety_margin_accuracy = self._evaluate_safety_margin_prediction(features_df)
        accuracy_results['safety_margin'] = safety_margin_accuracy
        
        # 4. Risk Stratification Accuracy
        print(f"\n4ï¸âƒ£ CLINICAL RISK STRATIFICATION:")
        risk_stratification_accuracy = self._evaluate_risk_stratification(features_df)
        accuracy_results['risk_stratification'] = risk_stratification_accuracy
        
        # 5. Hepatotoxicity Probability Accuracy
        print(f"\n5ï¸âƒ£ HEPATOTOXICITY PROBABILITY:")
        hepatotox_accuracy = self._evaluate_hepatotoxicity_probability(features_df)
        accuracy_results['hepatotoxicity_probability'] = hepatotox_accuracy
        
        # 6. Comparison with Previous Approaches
        print(f"\n6ï¸âƒ£ COMPARISON WITH PREVIOUS APPROACHES:")
        comparison_results = self._compare_with_previous_approaches(features_df)
        accuracy_results['approach_comparison'] = comparison_results
        
        self.accuracy_results = accuracy_results
        return accuracy_results
    
    def _evaluate_dili_classification(self, features_df: pd.DataFrame) -> Dict:
        """Evaluate DILI category classification accuracy."""
        try:
            # Create DILI category labels
            features_df['dili_category'] = features_df['dili_score'].apply(self._score_to_category)
            
            # Prepare features
            feature_cols = [col for col in features_df.columns 
                           if col.startswith(('oxygen_', 'hepato_', 'concentration', 'log_'))]
            
            X = features_df[feature_cols].fillna(0)
            X = X.loc[:, X.std() > 1e-6]
            y = features_df['dili_category']
            groups = features_df['drug']
            
            if len(X.columns) < 3 or y.nunique() < 2:
                return {'error': 'Insufficient data for classification'}
            
            # Cross-validation
            logo = LeaveOneGroupOut()
            model = RandomForestClassifier(n_estimators=100, random_state=42)
            
            # Get predictions for detailed metrics
            predictions = []
            actuals = []
            
            for train_idx, test_idx in logo.split(X, y, groups):
                X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]
                y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]
                
                model.fit(X_train, y_train)
                pred = model.predict(X_test)
                
                predictions.extend(pred)
                actuals.extend(y_test)
            
            # Calculate metrics
            accuracy = accuracy_score(actuals, predictions)
            precision = precision_score(actuals, predictions, average='weighted', zero_division=0)
            recall = recall_score(actuals, predictions, average='weighted', zero_division=0)
            f1 = f1_score(actuals, predictions, average='weighted', zero_division=0)
            
            print(f"  âœ… DILI Classification Accuracy: {accuracy:.3f}")
            print(f"     Precision: {precision:.3f}, Recall: {recall:.3f}, F1: {f1:.3f}")
            
            return {
                'accuracy': accuracy,
                'precision': precision,
                'recall': recall,
                'f1_score': f1,
                'n_samples': len(actuals),
                'n_drugs': features_df['drug'].nunique(),
                'classification_report': classification_report(actuals, predictions, output_dict=True)
            }
        
        except Exception as e:
            print(f"  âŒ Error in DILI classification: {str(e)[:50]}")
            return {'error': str(e)}
    
    def _evaluate_threshold_prediction(self, features_df: pd.DataFrame) -> Dict:
        """Evaluate concentration threshold prediction accuracy."""
        try:
            # Calculate actual thresholds for each drug
            threshold_data = []
            
            for drug in features_df['drug'].unique():
                drug_data = features_df[features_df['drug'] == drug].copy()
                
                if len(drug_data) < 4:
                    continue
                
                # Find threshold where CV increases significantly
                drug_data = drug_data.sort_values('concentration')
                baseline_cv = drug_data['oxygen_cv'].iloc[0] if 'oxygen_cv' in drug_data.columns else 0
                
                for _, row in drug_data.iterrows():
                    if 'oxygen_cv' in drug_data.columns and row['oxygen_cv'] > baseline_cv * 1.5:
                        threshold_data.append({
                            'drug': drug,
                            'actual_threshold': row['concentration'],
                            'log_threshold': np.log10(row['concentration'] + 1e-12),
                            'dili_score': row['dili_score']
                        })
                        break
            
            if len(threshold_data) < 5:
                return {'error': 'Insufficient threshold data'}
            
            threshold_df = pd.DataFrame(threshold_data)
            
            # Prepare features for threshold prediction
            feature_cols = [col for col in features_df.columns 
                           if col.startswith(('oxygen_', 'hepato_')) and 'threshold' not in col]
            
            # Aggregate features by drug
            drug_features = features_df.groupby('drug')[feature_cols].mean()
            
            # Merge with thresholds
            merged_data = threshold_df.merge(drug_features, left_on='drug', right_index=True, how='inner')
            
            if len(merged_data) < 5:
                return {'error': 'Insufficient merged data'}
            
            X = merged_data[feature_cols].fillna(0)
            y = merged_data['log_threshold']
            groups = merged_data['drug']
            
            # Cross-validation for threshold prediction
            logo = LeaveOneGroupOut()
            model = RandomForestRegressor(n_estimators=100, random_state=42)
            
            r2_scores = cross_val_score(model, X, y, cv=logo, groups=groups, scoring='r2')
            mae_scores = -cross_val_score(model, X, y, cv=logo, groups=groups, scoring='neg_mean_absolute_error')
            
            r2_mean = r2_scores.mean()
            mae_mean = mae_scores.mean()
            
            print(f"  âœ… Threshold Prediction RÂ²: {r2_mean:.3f}")
            print(f"     MAE: {mae_mean:.3f} log units")
            
            return {
                'r2': r2_mean,
                'r2_std': r2_scores.std(),
                'mae': mae_mean,
                'mae_std': mae_scores.std(),
                'n_samples': len(merged_data),
                'n_drugs': len(merged_data)
            }
        
        except Exception as e:
            print(f"  âŒ Error in threshold prediction: {str(e)[:50]}")
            return {'error': str(e)}
    
    def _evaluate_safety_margin_prediction(self, features_df: pd.DataFrame) -> Dict:
        """Evaluate safety margin prediction accuracy."""
        try:
            # Filter data with Cmax information
            cmax_data = features_df[features_df['cmax'].notna() & (features_df['cmax'] > 0)].copy()
            
            if len(cmax_data) < 10:
                return {'error': 'Insufficient Cmax data'}
            
            # Calculate safety margins (concentration / Cmax)
            cmax_data['safety_margin'] = cmax_data['concentration'] / cmax_data['cmax']
            cmax_data['log_safety_margin'] = np.log10(cmax_data['safety_margin'] + 1e-12)
            
            # Prepare features
            feature_cols = [col for col in cmax_data.columns 
                           if col.startswith(('oxygen_', 'hepato_', 'concentration_')) 
                           and 'safety_margin' not in col and 'cmax' not in col]
            
            X = cmax_data[feature_cols].fillna(0)
            X = X.loc[:, X.std() > 1e-6]
            y = cmax_data['log_safety_margin']
            groups = cmax_data['drug']
            
            if len(X.columns) < 3:
                return {'error': 'Insufficient features for safety margin prediction'}
            
            # Cross-validation
            logo = LeaveOneGroupOut()
            model = RandomForestRegressor(n_estimators=100, random_state=42)
            
            r2_scores = cross_val_score(model, X, y, cv=logo, groups=groups, scoring='r2')
            mae_scores = -cross_val_score(model, X, y, cv=logo, groups=groups, scoring='neg_mean_absolute_error')
            
            r2_mean = r2_scores.mean()
            mae_mean = mae_scores.mean()
            
            print(f"  âœ… Safety Margin Prediction RÂ²: {r2_mean:.3f}")
            print(f"     MAE: {mae_mean:.3f} log units")
            
            return {
                'r2': r2_mean,
                'r2_std': r2_scores.std(),
                'mae': mae_mean,
                'mae_std': mae_scores.std(),
                'n_samples': len(cmax_data),
                'n_drugs': cmax_data['drug'].nunique()
            }
        
        except Exception as e:
            print(f"  âŒ Error in safety margin prediction: {str(e)[:50]}")
            return {'error': str(e)}
    
    def _evaluate_risk_stratification(self, features_df: pd.DataFrame) -> Dict:
        """Evaluate clinical risk stratification accuracy."""
        try:
            # Create risk categories based on integrated DILI risk
            if 'integrated_dili_risk' not in features_df.columns:
                return {'error': 'No integrated DILI risk scores available'}
            
            features_df['risk_category'] = pd.cut(
                features_df['integrated_dili_risk'], 
                bins=[0, 40, 70, 100], 
                labels=['Low', 'Moderate', 'High']
            )
            
            # Remove NaN categories
            risk_data = features_df.dropna(subset=['risk_category'])
            
            if len(risk_data) < 10 or risk_data['risk_category'].nunique() < 2:
                return {'error': 'Insufficient risk category data'}
            
            # Prepare features
            feature_cols = [col for col in risk_data.columns 
                           if col.startswith(('oxygen_', 'hepato_', 'concentration')) 
                           and 'risk' not in col]
            
            X = risk_data[feature_cols].fillna(0)
            X = X.loc[:, X.std() > 1e-6]
            y = risk_data['risk_category']
            groups = risk_data['drug']
            
            # Cross-validation
            logo = LeaveOneGroupOut()
            model = RandomForestClassifier(n_estimators=100, random_state=42)
            
            # Get predictions
            predictions = []
            actuals = []
            
            for train_idx, test_idx in logo.split(X, y, groups):
                X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]
                y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]
                
                model.fit(X_train, y_train)
                pred = model.predict(X_test)
                
                predictions.extend(pred)
                actuals.extend(y_test)
            
            accuracy = accuracy_score(actuals, predictions)
            precision = precision_score(actuals, predictions, average='weighted', zero_division=0)
            recall = recall_score(actuals, predictions, average='weighted', zero_division=0)
            f1 = f1_score(actuals, predictions, average='weighted', zero_division=0)
            
            print(f"  âœ… Risk Stratification Accuracy: {accuracy:.3f}")
            print(f"     Precision: {precision:.3f}, Recall: {recall:.3f}, F1: {f1:.3f}")
            
            return {
                'accuracy': accuracy,
                'precision': precision,
                'recall': recall,
                'f1_score': f1,
                'n_samples': len(actuals),
                'n_drugs': risk_data['drug'].nunique()
            }
        
        except Exception as e:
            print(f"  âŒ Error in risk stratification: {str(e)[:50]}")
            return {'error': str(e)}
    
    def _evaluate_hepatotoxicity_probability(self, features_df: pd.DataFrame) -> Dict:
        """Evaluate hepatotoxicity probability prediction accuracy."""
        try:
            if 'hepatotoxicity_probability' not in features_df.columns:
                return {'error': 'No hepatotoxicity probability data'}
            
            # Prepare features
            feature_cols = [col for col in features_df.columns 
                           if col.startswith(('oxygen_', 'hepato_', 'concentration')) 
                           and 'probability' not in col]
            
            X = features_df[feature_cols].fillna(0)
            X = X.loc[:, X.std() > 1e-6]
            y = features_df['hepatotoxicity_probability']
            groups = features_df['drug']
            
            # Cross-validation
            logo = LeaveOneGroupOut()
            model = RandomForestRegressor(n_estimators=100, random_state=42)
            
            r2_scores = cross_val_score(model, X, y, cv=logo, groups=groups, scoring='r2')
            mae_scores = -cross_val_score(model, X, y, cv=logo, groups=groups, scoring='neg_mean_absolute_error')
            
            r2_mean = r2_scores.mean()
            mae_mean = mae_scores.mean()
            
            print(f"  âœ… Hepatotoxicity Probability RÂ²: {r2_mean:.3f}")
            print(f"     MAE: {mae_mean:.3f}")
            
            return {
                'r2': r2_mean,
                'r2_std': r2_scores.std(),
                'mae': mae_mean,
                'mae_std': mae_scores.std(),
                'n_samples': len(features_df),
                'n_drugs': features_df['drug'].nunique()
            }
        
        except Exception as e:
            print(f"  âŒ Error in hepatotoxicity probability: {str(e)[:50]}")
            return {'error': str(e)}
    
    def _compare_with_previous_approaches(self, features_df: pd.DataFrame) -> Dict:
        """Compare accuracy with previous classification approaches."""
        print(f"  ðŸ“Š COMPARISON WITH PREVIOUS METHODS:")
        
        comparison_results = {
            # Historical results from previous analyses
            'binary_classification': {
                'dili_accuracy': 0.720,
                'likelihood_accuracy': 0.560,
                'description': 'Binary DILI classification (Most/Less vs No concern)'
            },
            'multi_class_classification': {
                'dili_accuracy': 0.500,
                'likelihood_accuracy': 0.417,
                'description': 'Multi-class DILI classification (4 categories)'
            },
            'continuous_regression': {
                'dili_r2': 0.650,
                'likelihood_r2': 0.580,
                'description': 'Continuous toxicity scores (0-100 scale)'
            }
        }
        
        # Current integrated approach
        current_dili_acc = self.accuracy_results.get('dili_classification', {}).get('accuracy', 0)
        current_risk_acc = self.accuracy_results.get('risk_stratification', {}).get('accuracy', 0)
        current_threshold_r2 = self.accuracy_results.get('threshold_prediction', {}).get('r2', 0)
        
        comparison_results['integrated_concentration_approach'] = {
            'dili_accuracy': current_dili_acc,
            'risk_stratification_accuracy': current_risk_acc,
            'threshold_prediction_r2': current_threshold_r2,
            'description': 'Integrated DILI-concentration prediction system'
        }
        
        print(f"    â€¢ Binary Classification: {comparison_results['binary_classification']['dili_accuracy']:.3f}")
        print(f"    â€¢ Multi-class Classification: {comparison_results['multi_class_classification']['dili_accuracy']:.3f}")
        print(f"    â€¢ Continuous Regression: RÂ² = {comparison_results['continuous_regression']['dili_r2']:.3f}")
        print(f"    â€¢ Integrated Approach: {current_dili_acc:.3f} (DILI), {current_risk_acc:.3f} (Risk), RÂ² = {current_threshold_r2:.3f} (Thresholds)")
        
        return comparison_results
    
    def _score_to_category(self, score: float) -> str:
        """Convert DILI score to category."""
        if score <= 20:
            return 'No-Concern'
        elif score <= 50:
            return 'Ambiguous'
        elif score <= 70:
            return 'Less-Concern'
        else:
            return 'Most-Concern'
    
    def create_accuracy_dashboard(self, accuracy_results: Dict):
        """Create comprehensive accuracy visualization dashboard."""
        fig = plt.figure(figsize=(20, 14))
        gs = fig.add_gridspec(3, 4, hspace=0.4, wspace=0.3)
        
        # 1. Overall Accuracy Comparison (top left)
        ax1 = fig.add_subplot(gs[0, :2])
        
        methods = ['Binary\nClassification', 'Multi-class\nClassification', 
                  'Continuous\nRegression', 'Integrated\nConcentration']
        accuracies = [0.720, 0.500, 0.650, 
                     accuracy_results.get('dili_classification', {}).get('accuracy', 0)]
        
        colors = ['lightblue', 'lightgreen', 'orange', 'darkred']
        bars = ax1.bar(methods, accuracies, color=colors, alpha=0.8, edgecolor='black')
        
        ax1.set_ylabel('Accuracy / RÂ² Score')
        ax1.set_title('DILI Prediction Accuracy Comparison', fontweight='bold')
        ax1.set_ylim(0, 1.0)
        ax1.grid(True, alpha=0.3, axis='y')
        
        # Add value labels
        for bar, acc in zip(bars, accuracies):
            ax1.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.02,
                    f'{acc:.3f}', ha='center', va='bottom', fontweight='bold')
        
        # 2. Detailed Performance Metrics (top right)
        ax2 = fig.add_subplot(gs[0, 2:])
        
        metrics_data = []
        for task, results in accuracy_results.items():
            if 'error' not in results and task != 'approach_comparison':
                if 'accuracy' in results:
                    metrics_data.append({
                        'Task': task.replace('_', ' ').title(),
                        'Metric': 'Accuracy',
                        'Value': results['accuracy']
                    })
                if 'r2' in results:
                    metrics_data.append({
                        'Task': task.replace('_', ' ').title(),
                        'Metric': 'RÂ²',
                        'Value': results['r2']
                    })
                if 'f1_score' in results:
                    metrics_data.append({
                        'Task': task.replace('_', ' ').title(),
                        'Metric': 'F1-Score',
                        'Value': results['f1_score']
                    })
        
        if metrics_data:
            metrics_df = pd.DataFrame(metrics_data)
            
            # Create heatmap
            pivot_df = metrics_df.pivot(index='Task', columns='Metric', values='Value')
            
            sns.heatmap(pivot_df, annot=True, cmap='RdYlGn', center=0.5, 
                       ax=ax2, cbar_kws={'label': 'Performance Score'})
            ax2.set_title('Detailed Performance Metrics', fontweight='bold')
        
        # 3. Sample Size Analysis (middle left)
        ax3 = fig.add_subplot(gs[1, :2])
        
        sample_sizes = []
        task_names = []
        
        for task, results in accuracy_results.items():
            if 'n_samples' in results and 'error' not in results:
                sample_sizes.append(results['n_samples'])
                task_names.append(task.replace('_', ' ').title())
        
        if sample_sizes:
            bars = ax3.bar(task_names, sample_sizes, color='skyblue', alpha=0.8, edgecolor='black')
            ax3.set_ylabel('Number of Samples')
            ax3.set_title('Sample Sizes by Prediction Task', fontweight='bold')
            ax3.tick_params(axis='x', rotation=45)
            ax3.grid(True, alpha=0.3, axis='y')
            
            # Add value labels
            for bar, size in zip(bars, sample_sizes):
                ax3.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 1,
                        f'{size}', ha='center', va='bottom', fontweight='bold')
        
        # 4. Error Analysis (middle right)
        ax4 = fig.add_subplot(gs[1, 2:])
        
        mae_values = []
        mae_tasks = []
        
        for task, results in accuracy_results.items():
            if 'mae' in results and 'error' not in results:
                mae_values.append(results['mae'])
                mae_tasks.append(task.replace('_', ' ').title())
        
        if mae_values:
            bars = ax4.bar(mae_tasks, mae_values, color='lightcoral', alpha=0.8, edgecolor='black')
            ax4.set_ylabel('Mean Absolute Error')
            ax4.set_title('Prediction Error Analysis', fontweight='bold')
            ax4.tick_params(axis='x', rotation=45)
            ax4.grid(True, alpha=0.3, axis='y')
            
            # Add value labels
            for bar, mae in zip(bars, mae_values):
                ax4.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.01,
                        f'{mae:.3f}', ha='center', va='bottom', fontweight='bold')
        
        # 5. Accuracy Summary Report (bottom)
        ax5 = fig.add_subplot(gs[2, :])
        ax5.axis('off')
        
        # Generate summary statistics
        total_samples = sum(r.get('n_samples', 0) for r in accuracy_results.values() if 'error' not in r)
        total_drugs = max(r.get('n_drugs', 0) for r in accuracy_results.values() if 'error' not in r)
        
        avg_accuracy = np.mean([r.get('accuracy', 0) for r in accuracy_results.values() 
                               if 'accuracy' in r and 'error' not in r])
        avg_r2 = np.mean([r.get('r2', 0) for r in accuracy_results.values() 
                         if 'r2' in r and 'error' not in r])
        
        summary_text = f"""ðŸ“Š INTEGRATED DILI-CONCENTRATION ACCURACY SUMMARY

ðŸŽ¯ OVERALL PERFORMANCE:
â€¢ Average Classification Accuracy: {avg_accuracy:.3f} ({avg_accuracy*100:.1f}%)
â€¢ Average Regression RÂ²: {avg_r2:.3f} (explains {avg_r2*100:.1f}% of variance)
â€¢ Total Sample Size: {total_samples} drug-concentration combinations
â€¢ Drug Coverage: {total_drugs} unique compounds

ðŸ“ˆ TASK-SPECIFIC RESULTS:"""

        for task, results in accuracy_results.items():
            if 'error' not in results and task != 'approach_comparison':
                if 'accuracy' in results:
                    summary_text += f"""
â€¢ {task.replace('_', ' ').title()}: {results['accuracy']:.3f} accuracy ({results.get('n_samples', 0)} samples)"""
                elif 'r2' in results:
                    summary_text += f"""
â€¢ {task.replace('_', ' ').title()}: RÂ² = {results['r2']:.3f} ({results.get('n_samples', 0)} samples)"""

        summary_text += f"""

ðŸ† COMPARISON WITH PREVIOUS APPROACHES:
â€¢ Binary Classification: 72.0% â†’ Integrated: {accuracy_results.get('dili_classification', {}).get('accuracy', 0)*100:.1f}%
â€¢ Multi-class Classification: 50.0% â†’ Risk Stratification: {accuracy_results.get('risk_stratification', {}).get('accuracy', 0)*100:.1f}%
â€¢ Continuous Regression: RÂ² = 0.650 â†’ Threshold Prediction: RÂ² = {accuracy_results.get('threshold_prediction', {}).get('r2', 0):.3f}

ðŸ’¡ KEY IMPROVEMENTS:
â€¢ Concentration-specific predictions enable precise dosing guidance
â€¢ Safety margin assessment provides quantitative clinical thresholds
â€¢ Integrated risk scoring combines multiple hepatotoxicity factors
â€¢ Cross-validation ensures robust, generalizable performance

âš ï¸  LIMITATIONS:
â€¢ Limited by organoid model representation of human liver
â€¢ Requires clinical validation with patient outcome data
â€¢ Performance varies by drug class and mechanism of action
â€¢ Sample size constraints for some prediction tasks"""
        
        ax5.text(0.02, 0.98, summary_text, transform=ax5.transAxes,
                fontsize=11, verticalalignment='top', fontfamily='monospace',
                bbox=dict(boxstyle='round,pad=0.5', facecolor='lightyellow', alpha=0.8))
        
        plt.suptitle('ðŸŽ¯ DILI-CONCENTRATION PREDICTION ACCURACY ASSESSMENT', 
                     fontsize=18, fontweight='bold')
        
        # Save figure
        output_path = Path('results/figures/dili_concentration_accuracy_assessment.png')
        output_path.parent.mkdir(parents=True, exist_ok=True)
        plt.savefig(output_path, dpi=300, bbox_inches='tight', facecolor='white')
        plt.show()
        
        print(f"\nâœ… Accuracy assessment dashboard saved to: {output_path}")


def main():
    """Run comprehensive accuracy evaluation."""
    evaluator = DILIConcentrationAccuracyEvaluator()
    
    # Run comprehensive accuracy evaluation
    accuracy_results = evaluator.evaluate_comprehensive_accuracy(max_drugs=25)
    
    # Create accuracy dashboard
    evaluator.create_accuracy_dashboard(accuracy_results)
    
    # Print final summary
    print("\n" + "="*70)
    print("ðŸ† FINAL ACCURACY ASSESSMENT SUMMARY")
    print("="*70)
    
    if accuracy_results:
        for task, results in accuracy_results.items():
            if 'error' not in results and task != 'approach_comparison':
                print(f"\n{task.replace('_', ' ').title()}:")
                if 'accuracy' in results:
                    print(f"  â€¢ Accuracy: {results['accuracy']:.3f} ({results['accuracy']*100:.1f}%)")
                if 'r2' in results:
                    print(f"  â€¢ RÂ²: {results['r2']:.3f}")
                if 'mae' in results:
                    print(f"  â€¢ MAE: {results['mae']:.3f}")
                if 'n_samples' in results:
                    print(f"  â€¢ Samples: {results['n_samples']}")


if __name__ == "__main__":
    main()